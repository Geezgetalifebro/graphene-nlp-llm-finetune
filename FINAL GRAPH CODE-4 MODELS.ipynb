{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBN2Qo46Ml4i"
      },
      "outputs": [],
      "source": [
        "# === Install Dependencies ===\n",
        "!pip install --upgrade torch==2.4.0 transformers==4.44.2 datasets evaluate rouge_score bert_score sentence-transformers matplotlib -q\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --upgrade --force-reinstall\n",
        "!pip install transformers==4.35.2 --upgrade --force-reinstall\n",
        "!pip install accelerate --upgrade\n",
        "!pip install --upgrade transformers\n",
        "!pip install transformers==<specific_version>\n",
        "!pip cache purge\n",
        "\n",
        "\n",
        "# === Imports ===\n",
        "from datasets import Dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq, EvalPrediction\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "from bert_score import score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import gc\n",
        "import os\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import torchvision\n",
        "import torchaudio\n",
        "\n",
        "# === Suppress Warnings ===\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# === Set Environment Variable for Memory Management ===\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# === Clear GPU Memory Initially ===\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# === Memory Profiling ===\n",
        "def print_memory():\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated() / (1024*3):.2f} GiB, Reserved: {torch.cuda.memory_reserved() / (1024*3):.2f} GiB\")\n",
        "\n",
        "print(\"Initial Memory:\")\n",
        "print_memory()\n",
        "\n",
        "# === Load and Preprocess Datasets ===\n",
        "def load_datasets():\n",
        "    abstract_df = pd.read_csv(\"abstract_summaries_gptstyle.csv\")[[\"text\", \"summary\"]].dropna()\n",
        "    abstract_df = abstract_df[abstract_df[\"text\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    abstract_df = abstract_df[abstract_df[\"summary\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    abstract_df = abstract_df.rename(columns={\"text\": \"input_text\", \"summary\": \"target_text\"})\n",
        "    abstract_df[\"input_text\"] = \"summarize: \" + abstract_df[\"input_text\"]\n",
        "\n",
        "    qa_df = pd.read_csv(\"qanda.csv\", encoding='latin-1')[[\"Question\", \"Answer\"]].dropna()\n",
        "    qa_df = qa_df[qa_df[\"Question\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    qa_df = qa_df[qa_df[\"Answer\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    qa_df = qa_df.rename(columns={\"Question\": \"input_text\", \"Answer\": \"target_text\"})\n",
        "    qa_df[\"input_text\"] = \"question: \" + qa_df[\"input_text\"]\n",
        "\n",
        "    combined_df = pd.concat([abstract_df, qa_df], ignore_index=True)\n",
        "    dataset = Dataset.from_pandas(combined_df)\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = dataset[\"train\"]\n",
        "    test_dataset = dataset[\"test\"]\n",
        "\n",
        "    del abstract_df, combined_df, dataset\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return train_dataset, test_dataset, qa_df\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset, test_dataset, qa_df = load_datasets()\n",
        "print(\"After data loading:\")\n",
        "print_memory()\n",
        "\n",
        "# === Load 1000 Q&A Pairs for Evaluation ===\n",
        "def load_eval_dataset(qa_df, num_samples=1000):\n",
        "    qa_df = qa_df[qa_df[\"input_text\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    qa_df = qa_df[qa_df[\"target_text\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
        "    if len(qa_df) < num_samples:\n",
        "        eval_df = qa_df.sample(n=len(qa_df), random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        eval_df = qa_df.sample(n=num_samples, random_state=42).reset_index(drop=True)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "    return eval_dataset\n",
        "\n",
        "print(\"Loading 1000 Q&A pairs for evaluation...\")\n",
        "eval_dataset = load_eval_dataset(qa_df)\n",
        "print(f\"Selected {len(eval_dataset)} Q&A pairs for evaluation.\")\n",
        "print(\"After eval dataset loading:\")\n",
        "print_memory()\n",
        "\n",
        "# === Model Configurations ===\n",
        "model_configs = {\n",
        "    \"bart\": {\n",
        "        \"name\": \"facebook/bart-base\",\n",
        "        \"tokenizer_class\": BartTokenizer,\n",
        "        \"model_class\": BartForConditionalGeneration,\n",
        "        \"output_dir\": \"./bart-multitask-finetuned\",\n",
        "        \"color\": \"limegreen\"\n",
        "    },\n",
        "    \"distilbart\": {\n",
        "        \"name\": \"sshleifer/distilbart-cnn-6-6\",\n",
        "        \"tokenizer_class\": BartTokenizer,\n",
        "        \"model_class\": BartForConditionalGeneration,\n",
        "        \"output_dir\": \"./distilbart-multitask-finetuned\",\n",
        "        \"color\": \"magenta\"\n",
        "    },\n",
        "    \"pegasus\": {\n",
        "        \"name\": \"google/pegasus-xsum\",\n",
        "        \"tokenizer_class\": PegasusTokenizer,\n",
        "        \"model_class\": PegasusForConditionalGeneration,\n",
        "        \"output_dir\": \"./pegasus-multitask-finetuned\",\n",
        "        \"color\": \"gold\"\n",
        "    },\n",
        "    \"t5\": {\n",
        "        \"name\": \"t5-small\",\n",
        "        \"tokenizer_class\": T5Tokenizer,\n",
        "        \"model_class\": T5ForConditionalGeneration,\n",
        "        \"output_dir\": \"./t5-multitask-finetuned\",\n",
        "        \"color\": \"cyan\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# === Preprocess Function ===\n",
        "def preprocess_function(examples, tokenizer, max_input_length=128, max_target_length=32, is_t5=False):\n",
        "    inputs = examples[\"input_text\"]\n",
        "    targets = examples[\"target_text\"]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_input_length\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_target_length\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    if not is_t5:\n",
        "        labels = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# === Custom Trainer ===\n",
        "class CustomTrainer(Trainer):\n",
        "    def predict(self, test_dataset, ignore_keys=None):\n",
        "        self.model.eval()\n",
        "        data_loader = self.get_eval_dataloader(test_dataset)\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        max_length = 32\n",
        "\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(self.model.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.model.device)\n",
        "            batch_labels = batch['labels'].to(self.model.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_length=max_length,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "            padded_outputs = torch.nn.functional.pad(\n",
        "                outputs,\n",
        "                (0, max_length - outputs.size(1)),\n",
        "                value=self.tokenizer.pad_token_id\n",
        "            )\n",
        "            predictions.extend(padded_outputs.cpu().numpy())\n",
        "            labels.extend(batch_labels.cpu().numpy())\n",
        "            del input_ids, attention_mask, batch_labels, outputs, padded_outputs\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        labels = np.array(labels)\n",
        "        metrics = self.compute_metrics(EvalPrediction(predictions=predictions, label_ids=labels))\n",
        "        return predictions, labels, metrics\n",
        "\n",
        "# === Compute Metrics ===\n",
        "def compute_rouge_metrics(eval_pred, tokenizer):\n",
        "    preds, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    return rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "# === Train and Evaluate Model ===\n",
        "def train_and_evaluate(model_name, config, train_dataset, eval_dataset):\n",
        "    print(f\"\\n=== Processing {model_name} ===\")\n",
        "    tokenizer = config[\"tokenizer_class\"].from_pretrained(config[\"name\"])\n",
        "    model = config[\"model_class\"].from_pretrained(config[\"name\"]).to('cuda')\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    is_t5 = model_name == \"t5\"\n",
        "    train_ds = train_dataset.map(\n",
        "        lambda x: preprocess_function(x, tokenizer, is_t5=is_t5),\n",
        "        batched=True,\n",
        "        remove_columns=[\"input_text\", \"target_text\"]\n",
        "    )\n",
        "    eval_ds = eval_dataset.map(\n",
        "        lambda x: preprocess_function(x, tokenizer, is_t5=is_t5),\n",
        "        batched=True,\n",
        "        remove_columns=[\"input_text\", \"target_text\"]\n",
        "    )\n",
        "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    eval_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=lambda x: compute_rouge_metrics(x, tokenizer)\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model_name}...\")\n",
        "    trainer.train()\n",
        "    model.save_pretrained(config[\"output_dir\"])\n",
        "    tokenizer.save_pretrained(config[\"output_dir\"])\n",
        "    print(f\"After training {model_name}:\")\n",
        "    print_memory()\n",
        "\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    predictions, labels, metrics = trainer.predict(eval_ds)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(np.where(labels != -100, labels, tokenizer.pad_token_id), skip_special_tokens=True)\n",
        "\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge1_scores = [rouge.compute(predictions=[pred], references=[ref])['rouge1'] for pred, ref in zip(decoded_preds, decoded_labels)]\n",
        "    P, R, F1 = score(decoded_preds, decoded_labels, lang=\"en\", model_type=\"roberta-base\", batch_size=32, verbose=True)\n",
        "    F1 = F1.numpy()\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    pred_embeddings = embedding_model.encode(decoded_preds, convert_to_tensor=True, batch_size=32)\n",
        "    label_embeddings = embedding_model.encode(decoded_labels, convert_to_tensor=True, batch_size=32)\n",
        "    cos_sim = util.cos_sim(pred_embeddings, label_embeddings).diag().cpu().numpy()\n",
        "    ppl = calculate_perplexity(model, eval_ds)\n",
        "\n",
        "    del model, tokenizer, trainer, train_ds, eval_ds\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"After evaluation {model_name}:\")\n",
        "    print_memory()\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": rouge1_scores,\n",
        "        \"bertscore_f1\": F1,\n",
        "        \"cosine_sim\": cos_sim,\n",
        "        \"perplexity\": ppl,\n",
        "        \"decoded_preds\": decoded_preds,\n",
        "        \"decoded_labels\": decoded_labels,\n",
        "        \"metrics\": metrics\n",
        "    }\n",
        "\n",
        "# === Perplexity Calculation ===\n",
        "def calculate_perplexity(model, dataset):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=2)\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(model.device)\n",
        "        attention_mask = batch['attention_mask'].to(model.device)\n",
        "        labels = batch['labels'].to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            valid_tokens = (labels != -100).sum().item()\n",
        "            total_loss += loss.item() * valid_tokens\n",
        "            total_tokens += valid_tokens\n",
        "        del input_ids, attention_mask, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    return math.exp(total_loss / total_tokens)\n",
        "\n",
        "\n",
        "# === Run All Models ===\n",
        "results = {}\n",
        "for model_name, config in model_configs.items():\n",
        "    results[model_name] = train_and_evaluate(model_name, config, train_dataset, eval_dataset)\n",
        "\n",
        "# === Compute Cumulative Averages ===\n",
        "def cumulative_avg(data):\n",
        "    return np.cumsum(data) / np.arange(1, len(data) + 1)\n",
        "\n",
        "# === Plot Comparative Graphs ===\n",
        "print(\"Generating comparative plots...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# ROUGE-1\n",
        "plt.subplot(2, 2, 1)\n",
        "for model_name, config in model_configs.items():\n",
        "    cum_rouge1 = cumulative_avg(results[model_name][\"rouge1\"])\n",
        "    plt.plot(cum_rouge1, label=f\"Fine-tuned {model_name.capitalize()}\", color=config[\"color\"])\n",
        "plt.title('Cumulative Avg ROUGE-1')\n",
        "plt.xlabel('Example Index')\n",
        "plt.ylabel('ROUGE-1')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# BERTScore F1\n",
        "plt.subplot(2, 2, 2)\n",
        "for model_name, config in model_configs.items():\n",
        "    cum_f1 = cumulative_avg(results[model_name][\"bertscore_f1\"])\n",
        "    plt.plot(cum_f1, label=f\"Fine-tuned {model_name.capitalize()}\", color=config[\"color\"])\n",
        "plt.title('Cumulative Avg BERTScore F1')\n",
        "plt.xlabel('Example Index')\n",
        "plt.ylabel('BERTScore F1')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Cosine Similarity\n",
        "plt.subplot(2, 2, 3)\n",
        "for model_name, config in model_configs.items():\n",
        "    cum_cos = cumulative_avg(results[model_name][\"cosine_sim\"])\n",
        "    plt.plot(cum_cos, label=f\"Fine-tuned {model_name.capitalize()}\", color=config[\"color\"])\n",
        "plt.title('Cumulative Avg Cosine Similarity')\n",
        "plt.xlabel('Example Index')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Perplexity\n",
        "plt.subplot(2, 2, 4)\n",
        "for model_name, config in model_configs.items():\n",
        "    ppl = results[model_name][\"perplexity\"]\n",
        "    plt.axhline(y=ppl, label=f\"Fine-tuned {model_name.capitalize()}\", color=config[\"color\"])\n",
        "plt.title('Aggregate Perplexity')\n",
        "plt.xlabel('Example Index')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/combined_metric_curves.png')\n",
        "plt.show()\n",
        "print(\"Plot saved to /content/combined_metric_curves.png\")\n",
        "\n",
        "# === Save Evaluation Results ===\n",
        "print(\"Saving evaluation results...\")\n",
        "combined_results = {}\n",
        "for model_name in model_configs:\n",
        "    combined_results[model_name] = {\n",
        "        \"rouge_finetuned\": results[model_name][\"metrics\"],\n",
        "        \"bertscore_finetuned\": {\n",
        "            \"F1\": np.mean(results[model_name][\"bertscore_f1\"])\n",
        "        },\n",
        "        \"cosine_sim_finetuned\": np.mean(results[model_name][\"cosine_sim\"]),\n",
        "        \"perplexity_finetuned\": results[model_name][\"perplexity\"]\n",
        "    }\n",
        "\n",
        "with open(\"/content/combined_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(combined_results, f, indent=4)\n",
        "print(\"Results saved to /content/combined_evaluation_results.json\")\n",
        "\n",
        "# === Final Memory Check ===\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Final Memory:\")\n",
        "print_memory()"
      ]
    }
  ]
}